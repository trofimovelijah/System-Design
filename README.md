# Дизайн для системы глобального дейтингового сервиса
Система предполагает аналог *Tinder*, *Bumble*, *Badoo*, etc. 
В максимально приближенной к действительности постановке задачи, это будет мобильное приложение и веб-интерфейс, представленные (и, соответственно, локализованные) на рынке практически всех стран мира, c ± 100M MAU (Monthly active users) и ±10M DAU (Daily active users).
Разработка дизайна предполагается по мере приращения требований: от Базового до Третьего уровня.

# Первый уровень: Базовый дизайн
## Функциональные требования
1. **Профили пользователей**:
   - пользователи могут создавать, сохранять и редактировать свои анкеты.
2. **Выставление оценки чужой анкете**: 
   - пользователям могут выставлять оценки другим анкетам (по принципу лайк/дизлайк).
3. **Доступ к приложению**:
   - просматривать профили и контент в них могут только зарегистрированные пользователи,
   - есть возможность блокировки неприятных анкет.
5. **Соответствие**: 
   - когда два пользователя выставляют положительные оценки друг другу, считается, что они подходят друг другу, поэтому для них активируется чат.
6. **Уведомления**: 
   - пользователи получают уведомления о совпадениях и сообщениях.
7. **Медиаконтент**: 
   - пользователи могут загружать фотографии и короткие видеоролики.
8. **Чат в режиме реального времени**: 
   - совпадающие пользователи могут общаться в режиме реального времени.

## Нефункциональные требования
1. **Масштабируемость** для обеспечения высокой нагрузки: 
   - поддержка ±100 миллионов MAU (ежемесячно активных пользователей) и ±10 миллионов DAU (ежедневно активных пользователей).
2. **Доступность**: 
   - время безотказной работы составляет 99,9%, иначе пользователи уйдут в другой сервис знакомств.
3. **Консистентность** (для чатов):
   - в чатах сообщения должны быть одинаковыми для всех пользователей, иначе случится драма и отказ в дальнейшем использовании сервиса
4. **Надёжность**:
   - надёжное хранение и передача пользовательских данных
5. **Устойчивость к разделению**:
   - географическая распределённость диктует необходимость 
  
## Расчёт нагрузки
Исходим из предположения, что наше приложение позволяет отображать на странице до 10 фотографий и изображений средним размером в 500 кБ, либо 1 фото и максимум 9 коротких полуминутных видеороликов, оптимизированных под конкретное устройство, с которого происходит просмотр анкеты. Остальная информация в анкете (описание, история лайков и т.п.) несоизмеримо меньше по размерам. Предполагаемый размер видеоролика рассчитывается из следующего: FullHD-видео (1080p) длиной в час весит 1,5 ГБ (~420кБ, округляем до 500 кБ). 
### Запросы
   - 10M пользователей * 50 * (30 * 9 + 1) (предполагается, что пользователь просматривает по 50 анкет за сутки, включая всё видео и фото профиля) / 86400 секунд = 2М RPS (грубо округляем в сторону целого операции на чтение),
   - 100K пользователей * 10 медиа (фото+видео) = 1М RPS (на запись).
### Сетевая нагрузка
   - 500кБ (размер секунды видео или одного изображения) * 2М (число RPS на чтение) = 1 Тб/с (суточный трафик),
   - 1 Тб/с * 86400 секунд * 366 дней = 32К Пб (годовой трафик).
### Хранение данных
   - 100М пользователей ежемесячно * (30 * 9 + 1) объём контента анкеты * 500 кБ * 12 месяцев = 163 Пб (в год)
### Вычислительные мощности
   - на выполнение чтение/запись согласно расчётам потребуется **500** инстансов,
   - если один инстанс может обрабатывать 10 Гб/с трафика, нам потребуется **100** инстансов для обработки сетевой нагрузки в 1 Тб/с,
   - для хранения данных в 163 Пб потребуется (с учётом резервирования дисков) ~ 41К инстансов (в год).
### Стоимость
   - стоимость сетевой нагрузки (выходящего трафика) в объеме 32 ПБ в год может варьироваться в зависимости от выбранного облачного провайдера:
      - AWS: около $2.24 миллионов в год,
      - Google Cloud: около $1.6 миллионов в год,
      - Azure: около $1.6 миллионов в год.
   - стоимость хранения данных объемом 163 ПБ в год может варьироваться в зависимости от выбранного облачного провайдера:
      - AWS (Amazon S3): около $44.99 миллионов в год,
      - Google Cloud Storage: около $39.12 миллионов в год,
      - Azure Blob Storage: около $35.99 миллионов в год.

## Компоненты и взаимодействия
1. User service управляет анкетами пользователей
2. Auth service - сервис аутентификации
3. Match service (сервис сопоставлений) управляет логикой "нравится", "не нравится" и "соответствует"
4. Notification service (сервис уведомлений) отправляет различные виды уведомлений (push, смс, email) о совпадениях и сообщениях
5. Media service позволяет осуществлять загрузку и просмотр медиаконтента (изображения, короткие видео)
6. Chat service управляет чатом между пользователями в режиме реального времени.

## Высокоуровневый дизайн
![изображение](https://github.com/trofimovelijah/System-Design/assets/15788014/32809fd3-2c07-4c39-8754-bd8d4cc36bd8)

## Хранилища и базы данных
1. Ведение анкеты в User service:
   - пользователи создают/редактируют профили. Сами данные чувствительны, поэтому их уместно хранить в реляционной СУБД, наподобие PostgreSQL.
2. Сопоставления в Match service:
   - действия пользователя регистрируются, а совпадения идентифицируются службой сопоставления. Такая база должна удовлетворять ACID-свойствам и к ней можно быстро осуществлять различные join'ы. Поэтому разумно использовать реляционную СУБД,
   - сами лайки/дизлайки уместно хранить в БД типа ключ-значение, например, Redis.
3. Уведомления:
   - запускаются при совпадениях, уведомления отправляются через службу push-уведомлений. Данные о логировании сообщений целесообразно хранить в колоночной БД, поскольку у нас всего три формата однородных уведомлений, а доступ к самой базе нужен преимущественно на чтение.
4. Медиаконтент:
   - загруженные медиа хранятся в распределенном файловом S3-хранилище.
5. Чат:
    - сообщения хранятся в документоориентированная NoSQL БД и извлекаются в режиме реального времени.
6. Аутентификация:
   - сервис хранит данные о регистрации, аутентификации и сессиях пользователей. Необходимо удовлетворять ACID-свойствам, поэтому оптимально применять реляционную СУБД. Возможно использовать базу сервиса User service, но для большой системы это небезопасно. При этом появляется вопрос синхронизации данных баз разных сервисов. 

## Компонентный дизайн
1. **Пользовательский доступ**:
   - User UI - анкета и оценки,
   - Auth UI - форма входа и регистрации,
   - Chat UI - переписка, 
   - Media UI - страница загрузки и просмотра медиаконтента, главным образом, видео.
2. В качестве брокера сообщений выбираем не RabbitMQ, а Apache Kafka, поскольку он оптимальнее подходит под задачи масштабирования, а фоновых задач, как таковых, кроме загрузки видео, пока не предвидится.
3. Добавлено отдельное хранилище для быстрых сообщений (лайк/дизлайк) в User service
4. Сервис управления медиаконтентом описан подробнее:
   - декомпозиция на сервис загрузки Media upload service и сервис просмотра Media view service,
   - загружаемые короткие видео разных форматов проходят перекодировку для оптимизации отображения и обрезки видео.
5. **Сервис уведомлений**: 
   - подключён к брокеру сообщений Apache Kafka,
   - три вида уведомлений отправляются по внешнему API,
   - отдельно выделено логирование уведомлений в колоночную БД.
6. Для обеспечения синхронизации между User service и Auth service также используется обмен сообщений через Kafka. Подобную конструкцию проще масштабировать, а также повышается надёжность, обеспечивая доставку даже в случае временных сбоев.
7. Взаимодействие других сервисов можно пустить также через очередь:
   - сервис аутентификации всегда оказывается продьюсером,
   - по отношению к нему сервис сообщений всегда консьюмер через брокер,
   - пользовательский сервис может быть как продьюсером для Chat service, так и консьюмером для Auth service,
   - сервис оповещений всегда консьюмер

![изображение](https://github.com/trofimovelijah/System-Design/assets/15788014/abac0228-bb44-4a37-a328-e8bbfb39df1c)

## Масштабирование системы и повышение надёжности
### Меры по масштабированию системы
Ранее проведённые расчёты нагрузки показали потребность в очень больших мощностях. 100М ежемесячных пользователей вызывает необходимость масштабировать систему. Рассмотрим возможные сценарии:
1. Необходимость распределять трафик с помощью балансировщиков нагрузки:
   - между пользовательским доступом и сервисами User service, Auth service, Chat service и двумя Media service,
   - ввиду большого числа рассчитанных инстансов сервисов целесообразно использовать балансировщики между ними,
   - при обращении к S3-хранилищу также нужно добавить LB.
2. Балансировщики можно снабдить резервными экземплярами в режиме Stand by.
3. Применение репликации данных:
   - для реляционных СУБД используется репликация,
   - для базы данных сервиса сообщений организована репликация методом кворума, поскольку это наиболее оптимальный способ для выполнения требования по консистентности данных в чатах,
   - хранилище S3 уместно разбить на партиции.
4. По подсчётам число инстансов сервисов может достигать порядка полутысячи. Для масштабирования нагрузки используются инстансы, которые в случае отказа основного инстанса подхватывают работу.

### Меры по улучшению отзывчивости
1. Первым способом повышения отзывчивости выступает кэширование данных. В зависимости от БД имеет смысл применить разный способ инвалидации кэша:
   - для сервиса уведомлений кэш использовать не будем, поскольку данный сервис не предполагает повышенных требований к отзывчивости (уведомление допустимо приходить через определённый промежуток времени),
   - для сервиса сравнений Match service уместно применить реверсивную запись кэша, поскольку этим обеспечивается высокая скорость записи,
   - сквозная запись кэша целесообразна для сервиса сообщений (в переписке данные должны быть консистентны),
   - у сервиса аутентификации кэш предполагается записывать в обход, поскольку логично его использование только после получения аутентификационных данных,
   - аналогично для базы данных анкет Profiles запись кэша в обход,
   - запись напрямую в источник также целесообразна для S3-хранилища,
   - быстрое хранилище Likes на Redis не имеет смысл кэшировать, поскольку эти данные хранятся в оперативной памяти, а долгосрочная статистика храниться в базе данных Profiles, поэтому оставим под вопросом.
2. Поскольку предполагается, что система должна быть географически распределена, то имеет смысл использовать CDN для ускорения отзыва:
   - использовать между пользовательскими точками входа, наподобие Chat UI (переписка может быть между людьми с разных географических и часовых зон),  и соответствующим сервисом,
   - для Media UI использование CDN целесообразно с сервисом Media view service.
3. Индексирование баз данных применяем для всех случаев, когда число операций чтения больше операций на запись:
   - для S3 применяем индексацию (под вопросом) со стороны сервиса Media view service, поскольку сам сервис предназначен для просмотра медиаконтента,
   - база сервиса аутентификации под вопросом, поскольку для входа в приложение выполняется условный POST-запрос, который данные записывает. Далее пользователь использует токен авторизации (на будущее - рассмотреть вопрос хранения токена в UI),
   - для Auth UI и User UI уместно использовать пользовательский кэш
4. Для повышения отзывчивости целесообразно применить установка соединения между клиентом и сервером по веб-протоколу HTTP:
   - для Chat UI (отправив сообщение, пользователь ждёт оперативной доставки ответа от собеседника) можно применить либо HTTP Long-Polling соединение, либо соединение по WebSocket,
   - от пользовательского сервиса ожидаются оповещения (например, поставил ли кто-нибудь лайк анкете пользователя), поэтому также применяем HTTP Long-Polling, 
   - в остальных случаях от User service и от клиента User UI можно отправлять данные по WebSocket.

## Итоговый дизайн системы Базового уровня
На данной схеме не учитываются дополнительные сервисы (мониторинг, безопасность, etc), поскольку они будут отображены в дизайне Третьего уровня.
![изображение](https://github.com/trofimovelijah/System-Design/assets/15788014/cb5a1705-6143-49e4-acc7-43f9be262b7c)


# Второй уровень: Фильтрация по геолокации и предпочтениям
## Функциональные требования
1. **Поиск по геолокации**:
   - пользователи могут искать анкеты других пользователей в заданном радиусе от текущего местоположения,
   - поиск может осуществлять по конкретным местоположениям (возможно указать точку на карте).
2. **Фильтры предпочтений**:
   - фильтрация анкет на основе предпочтений пользователя (возраст, пол, интересы и т.д.).
3. **Соответствие** (дополнение):
   - сервис соответствий может подбирать анкеты в т.ч. с учётом географического положения.

## Нефункциональные требования
Помимо указанных ранее требований
1. **Масштабируемость** для обеспечения высокой нагрузки: 
   - система должна быть способна масштабироваться горизонтально для поддержки увеличивающегося числа пользователей,
   - данные должны быть шардированы для равномерного распределения нагрузки между серверами.
2. **Доступность**: 
   - время безотказной работы составляет 99,9%, иначе пользователи уйдут в другой сервис знакомств.
3. **Производительность**:
   - система должна поддерживать высокую производительность при больших нагрузках, обеспечивая быстрый поиск и фильтрацию анкет,
   - время отклика системы должно быть минимальным даже при большом количестве запросов.
4. **Надёжность**:
   - надёжное хранение и передача пользовательских данных.

## Компоненты и взаимодействия
1. Сервис геолокации Geolocation service:
   - хранит данные о местоположении в геопространственной базе данных,
   - предоставляет API для запроса пользователей по местоположению.
2. Сервис поиска и фильтрации GeoSearch service:
   - обрабатывает сложные запросы, объединяющие геолокацию и предпочтения,
   - индексирует профили в Elasticsearch.

## Высокоуровневый дизайн
![изображение](https://github.com/trofimovelijah/System-Design/assets/15788014/c5593a62-a03b-4a40-9890-f6f32d8af781)

## Хранилища и базы данных
1. Location service определяет услуги геолокации:
   - используется геопространственная БД (например, PostGIS) для хранения и запроса данных о местоположении,
   - используется стратегия сегментирования на основе дерева квадрантов или сетки, чтобы справиться с высокой плотностью пользователей в городах,
   - согласованность и синхронизацию между сегментами.
2. Служба поиска и фильтрации GeoSearch service:
   - использует Elasticsearch для быстрого запроса и фильтрации.
   - индексирует профили пользователей с помощью атрибутов геолокации и предпочтений.

## Компонентный дизайн

## Масштабирование системы и повышение надёжности

# Третий уровень: Обработка больших объемов данных и компонентов ML
## Дополнительные функциональные требования
1. **Сбор данных о событиях**:
   - фиксирует 1 млн событий в секунду.
2. **ML-механизм рекомендаций для сортировки профилей**:
   - сортировка отправляемых пользователям непросмотренных анкет, при которой увеличивается вероятность совпадений; сервис 
3. **ML-сервис Push-уведомлений для привлечения пользователей**:
   - рекомендации анкет или сервис промо-пушей, стимулирующих пользовательскую активность.

## Компоненты и взаимодействия
1. Обработчик потока событий Event Stream Processor :
- принимает и обрабатывает события в режиме реального времени.
- сохраняет обработанные данные в хранилище данных для аналитики.
2. Recommendation Service Служба рекомендаций:
- использует модели ML для ранжирования и сортировки профилей.
- извлекает функции из хранилища функций и обрабатывает рекомендации в режиме реального времени.
3. Служба Push-уведомлений:
- использует модели ML для прогнозирования и своевременной отправки push-уведомлений.
- отслеживает активность пользователей и запускает уведомления о привлечении.

## Высокоуровневый дизайн
1. Сбор данных о событиях и ETL:
   - платформа распределённой потоковой передачи событий (например, Apache Kafka) для обработки событий.
   - реализация конвейеров ETL для обработки и хранения данных в хранилище данных (например, Amazon Redshift).
2. Инфраструктура машинного обучения:
   - развёртывание модели ML с использованием масштабируемой платформы обслуживания (например, TensorFlow Serving).
   - использование хранилища функций для согласованного управления функциями ML.

## Хранилища и базы данных
1. Сбор событий: действия пользователя генерируют события, которые обрабатываются и сохраняются обработчиком потока событий.
2. Механизм рекомендаций: запрашивает пользовательские данные, ранжирует профили и предоставляет персонализированные рекомендации.
3. Push-уведомления: запускает уведомления на основе поведения пользователя и моделей взаимодействия.

## Компонентный дизайн

## Масштабирование системы и повышение надёжности

# Инфраструктура сбора и хранения данных, логов и телеметрии

# Требования по безопасности
Спроектировать подсистемы, связанные с безопасностью: управление ключами шифрования, идентификация юзеров, защита данных при хранении и передаче, хранение и проверка прав на доступ и т.д.

## Оценки и соображения
### Оценки нагрузки и трафика
1. 100M MAU и 10M DAU:
   - Учет пиковой нагрузки при тысячах запросов в секунду.
   - Высокая пропускная способность баз данных и систем хранения данных для чтения и записи.
### Оценка затрат
1. Затраты на трафик: Оценка на основе передачи данных и использования CDN.
2. Затраты на хранение: Для реляционных, NoSQL и мультимедийных хранилищ.
3. Вычислительные затраты: Для запуска серверных служб, моделей ML и процессов ETL.
### Reliability and Responsiveness Надежность и оперативность
1. Избыточность: Развертывание в нескольких регионах для аварийного восстановления.
2. Масштабируемость: Автоматическое масштабирование групп и бессерверные функции для обработки изменений нагрузки.
3. Мониторинг и оповещения: Используйте такие инструменты, как Prometheus и Grafana, для мониторинга и оповещения в режиме реального времени.
### Соображения безопасности
1. Шифрование данных: Шифруйте данные в состоянии покоя и при передаче.
2. Аутентификация и авторизация: Реализуйте надежную аутентификацию (например, OAuth2) и управление доступом на основе ролей.
3. Соответствие требованиям: Обеспечьте соблюдение правил защиты данных (например, GDPR, CCPA).

### Мониторинг и взаимодействие системы
